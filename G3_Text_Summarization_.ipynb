{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO5AktRVwvtG3psQjlGDeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keltaimpex/skills-introduction-to-github/blob/main/G3_Text_Summarization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk sklearn rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJk8uTeiOJWN",
        "outputId": "6cd8c309-9039-431d-d5e0-362635b3983a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2ZD_sT3pAHUL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Nu0MHEJRqf",
        "outputId": "2432f060-adbe-4996-bf86-cf14492cfb78"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", name=\"3.0.0\")"
      ],
      "metadata": {
        "id": "S29iQGifJcjm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset[\"train\"]"
      ],
      "metadata": {
        "id": "CZA0wN-LJ7mz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validations_dataset = dataset[\"validation\"]"
      ],
      "metadata": {
        "id": "8_h21BHjKDGo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize   #this is for splitting text into sentences and words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer    #this is a tool for calculating TF-IDF scoring methods."
      ],
      "metadata": {
        "id": "PxqebaH5NNNW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install rouge_score              #installing the module rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFTYZ7OgOqhe",
        "outputId": "d4abb557-9485-48b6-8b0f-1d0305a7d03e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer   #this is a tool fro evaluating summaries using ROUGE metrics"
      ],
      "metadata": {
        "id": "Av4giMB2O-mw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and setting up necessary nltk data"
      ],
      "metadata": {
        "id": "K_kDiWMPPjgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  #download tokenizer models for sentence and wordtokenization\n",
        "nltk.download('stopwords')  #download stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_fN8Xj1PZNv",
        "outputId": "dee3a29f-cba8-4187-f9c7-06598f89b1f5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading our dataset(we used the CNN/Daily Mail dataset"
      ],
      "metadata": {
        "id": "yS_66r6IQB-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"cnn_dailymail\", name=\"3.0.0\")"
      ],
      "metadata": {
        "id": "2KgaHf1jQAk8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the trainig data"
      ],
      "metadata": {
        "id": "IYk6H6ebQRW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data= dataset[\"train\"]"
      ],
      "metadata": {
        "id": "sD_dAP7gQV0f"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample data for demonstartion"
      ],
      "metadata": {
        "id": "gJYanoPzQeZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_doc=train_data[0]\n",
        "article=sample_doc[\"article\"]\n",
        "summary=sample_doc[\"highlights\"]"
      ],
      "metadata": {
        "id": "nsLCjf2nQi5b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing: Lowercase and splitting the text into sentences"
      ],
      "metadata": {
        "id": "MdH0BMRnQ2I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text.lower())    #convert to lowercase and split into sentences.\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "cMjQefZTQ82R"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Cleaning"
      ],
      "metadata": {
        "id": "ebpCxl8GRI7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentences(sentences):\n",
        "  stop_words= set(nltk.corpus.stopwords.words(\"english\"))   #set of English stopwords.\n",
        "  cleaned=[]\n",
        "  for sentence in sentences:\n",
        "    tokens=word_tokenize(sentence)   #spli sentence into tokens or words\n",
        "    tokens=[word for word in tokens if word.isalnum() and word not in stop_words]   #Remove stopwords and non-alphanumeric tokens\n",
        "    cleaned.append(' '.join(tokens))      #join tokens back into a cleaned sentence\n",
        "  return cleaned"
      ],
      "metadata": {
        "id": "BaFs2cNPRL8H"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the sample document"
      ],
      "metadata": {
        "id": "MitpVgYnSilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences=preprocess_text(article)\n",
        "cleaned_sentences=clean_sentences(preprocessed_sentences)   #cleaned sentences"
      ],
      "metadata": {
        "id": "ixQXtmTnSmhh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorizer setup"
      ],
      "metadata": {
        "id": "L6fGoWbQSu_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer=TfidfVectorizer()\n",
        "tfidf_matrix=tfidf_vectorizer.fit_transform([''.join(cleaned_sentences)])      #converts sentences to TF-IDF matrix"
      ],
      "metadata": {
        "id": "0FygoVc7S1BQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to summarize by selecting top N sentences"
      ],
      "metadata": {
        "id": "ZDnIHlmmTMO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_document(sentences, tfidf_matrix, top_n=2):\n",
        "  scores=tfidf_matrix.toarray().sum(axis=1)   #sum TF-IDF scores for each sentence\n",
        "  ranked_sentences=[sentence for score, sentence in sorted(zip(scores,sentences),reverse=True)]      #Rank sentences by score # use scores instead of score\n",
        "  return ranked_sentences[:top_n]      #Return top N sentences"
      ],
      "metadata": {
        "id": "PbcfJm5oTSo-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate summary for the sample document"
      ],
      "metadata": {
        "id": "pL2jhllgWZpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_sentences = summarize_document(cleaned_sentences, tfidf_matrix, top_n=2)\n",
        "summary = ' '.join(summary_sentences)   #Join selected sentences to form the summary"
      ],
      "metadata": {
        "id": "4k27_wdQWm9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now printing the results"
      ],
      "metadata": {
        "id": "CNVyonhuWyOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Article:\")\n",
        "print(article)\n",
        "print(\"\\nGenerated Summary:\")\n",
        "#print(summary_text)  # This line caused the error because summary_text was not defined\n",
        "print(summary)   # Assuming you want to print the summary calculated in the previous step\n",
        "print(\"\\nReference Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EZw41MoW161",
        "outputId": "4941cbb8-a87d-41cc-fe1e-0b992f6c8be7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Article:\n",
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
            "\n",
            "Generated Summary:\n",
            "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
            "\n",
            "Reference Summary:\n",
            "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate vthe generated summary using ROUGE metrics"
      ],
      "metadata": {
        "id": "eC9GLc0TXZyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer=rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'],use_stemmer=True)     #ROUGE scorer setup\n",
        "scores=scorer.score(summary,summary)     #calculate ROUGE scores  # Changed summary_text to summary\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zLHJ4dBXgE3",
        "outputId": "d903a62d-6d10-490f-c992-bc9f69dbacc0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores:\n",
            "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer(): Converts the text into numerical features based on word importance.\n",
        "\n",
        "sent_tokenize(): Splits the text into individual sentences.\n",
        "word_tokenize(): Splits sentences into individual words (tokens).\n",
        "stopwords: Commonly used words that are often removed from the text for summarization.\n",
        "\n",
        "\n",
        "datasets: For loading and accessing datasets.\n",
        "nltk: For text preprocessing tasks.\n",
        "sklearn: For calculating TF-IDF scores.\n",
        "rouge_score: For evaluating the quality of generated summaries."
      ],
      "metadata": {
        "id": "YRkyjsRBYLh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                Insights\n",
        "    Importance of Extractive Summarization\n",
        "\n",
        "Efficiency: Extractive summarization helps users quickly obtain key information from large texts without needing to read the entire content.\n",
        "Relevance: It ensures that the extracted sentences are directly from the source text, preserving the original context and content.\n",
        "Application in Information Retrieval: It is used in search engines, news aggregation services, and document management systems to enhance user experience by providing concise summaries of relevant information.\n",
        "\n",
        "     Challenges and Future Work\n",
        "\n",
        "Summary Coherence: Extractive methods may sometimes produce summaries that lack coherence due to the direct extraction of sentences.\n",
        "Expansion: Explore hybrid approaches that combine extractive and abstractive summarization methods to improve the quality of summaries."
      ],
      "metadata": {
        "id": "lA5Vcp1BcDLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "       Conclusion\n",
        "The extractive summarization system developed using the CNN/Daily Mail dataset demonstrates the capability to produce concise summaries by extracting important sentences. Evaluations using ROUGE metrics indicate the effectiveness of the approach, although there is room for improvement in terms of summary coherence and overall quality. Future work could involve integrating more advanced techniques and hybrid models to enhance summarization performance."
      ],
      "metadata": {
        "id": "VzVZCLYxbnbp"
      }
    }
  ]
}